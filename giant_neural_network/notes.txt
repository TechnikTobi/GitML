m* -> Measurements
w* -> weights
b -> bias



sigmoid-function -> Schaut aus wie ein in die Länge gestrechtes "S"
Wird benutzt, um m*w+b auf einen Wert größer 0 und kleiner 1 zu bringen
Formel: 1 / (1 + e **(-x))
Kurz: exp() (?)



cost function:
- Input: Targets, Predictions
- Output: Number; Sagt uns, wie gut/schlecht das NN arbeitet
- Ziel: Output minimieren, um Predictions besser zu machen
Definition: cost() = (prediction - target) ** 2

- Wie nutzt man die cost function um das NN zu verbessern?
bias anpassen:
	- NN() = b -> Prediction = b
	- cost function (b - target) ** 2 -> Parabel
	- Man beobachtet die Steigung der Funktion am verwendeten b:
		- ist Steigung positiv -> b muss kleiner werden
		- ist Steigung negativ -> b muss größer werden
	- Ziel: Steigung = 0
	- Dazu verwendet man die Funktion der Steigungsfunktion der cost function
	-> b = b - 0.1 * slope(b)
	=> slope(b) = ????

Slope of cost function:
3 Ways:
- Numerically
- Algebraically
- Calculus-ly

1. Numerically:
slope = rise / run
Man braucht 2 Punkte:
b = (b, cost(b))
h = (b+h, cost(b+h)) -> h = "Schrittweite"; je kleiner h desto genauer
-> h ist ein Comparison Point
=> 	rise = cost(b+h) - cost(b)
	run = b+h - b = h
	(Das hat doch was mit dem Steigungsdreieck zu tun! ;)
Positiv:
- Funktioniert mit allen Funktionstypen (Quadrat, sin, sigmoid, etc.)
Negativ:
- Resourcen unfreundlich, da cost() 2x berechnet werden muss
- Nicht ganz genau, nur ein Näherungswert

2. Algebraically
Expanding the slope function from the numerical way:
slope = (cost(b+h) - cost(b)) / h
	((b+h - 4)**2 - (b - 4)**2) / h
	(b**2 + h**2 + 4**2 + 2bh - 2*4b -2*4h - b**2 + 2*4b - 4**2) / h
	(h**2 + 2bh - 2*4h) / h
	h * (h + 2b - 2*4) / h
	h + 2b - 2*4
	2 * (b-4) + h
	Da die Genauigkeit besser wird je kleiner h ist, kann man h wegfallen lassen
	=> 2 * (b-4)
slope = 2 * (b-4) -> Da es kein h gibt ist es auch der genaue Wert und kein Näherungswert mehr

3. Calculus-ly
In Calculus (ACHTUNG: WURDE NOCH NICHT GELERNT):
	1. Potenz mit Exponent multiplizieren
	2. Exponent = Exponent - 1
=> "Power Rule"
Daraus folgt der gleiche Ausdruck wie mit dem Algebraischen Weg



Linear Regression:
Addieren der Cost Werte der einzelnen Target Werte:
Cost = sum(model(x_i) - target_i)**2
- x_i = measurments
- model(x_i) = prediction
- target_i = target for x_i
